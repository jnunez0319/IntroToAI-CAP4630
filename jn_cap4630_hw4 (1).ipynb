{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LwvvMtG18SK"
      },
      "source": [
        "# Recurrent Neural Network Homework\n",
        "\n",
        "This is the 4th assignment for CAP 4630 and we will implement a basic RNN network and an LSTM network with Keras to solve two problems. \\\n",
        "You will use **\"Tasks\"** and **\"Hints\"** to finish the work. **(Total 100 points, including 15 bonus points)** \\\n",
        "You may use Machine Learning libaries like Scikit-learn for data preprocessing.\n",
        "\n",
        "**Task Overview:**\n",
        "- Implement a basic RNN network to solve time series prediction \n",
        "- Implement an LSTM network to conduct sentiment analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l24oSrIK18SL"
      },
      "source": [
        "## 1 - Implement Basic RNN network with Keras to predict time series##\n",
        "### 1.1 Prepare the data (17 Points)\n",
        "\n",
        "Prepare time series data for deep neural network training.\n",
        "\n",
        "**Tasks:**\n",
        "1. Load the given train and test data: \"train.txt\" and \"test.txt\". **(5 Points)**\n",
        "2. Generate the **TRAIN** and **TEST** labels. **(5 Points)**\n",
        "3. Normalize the **TRAIN** and **TEST** data with sklearn function \"MinMaxScaler\". **(5 Points)**\n",
        "4. **PRINT OUT** the **TEST** data and label. **(2 Points)**\n",
        "\n",
        "**Hints:**  \n",
        "1. The length of original train data is 113 which starts from **\"1949-01\"** to **\"1958-05\"**. The length of original test data is 29, which starts from **\"1958-07\"** to **\"1960-11\"**. \n",
        "2. Set the data types of both train and test data to \"float32\". \n",
        "3. When you prepared input data X (sequences) and oupt data Y (labels), please consider the following relationship:\n",
        "    - The sequence X should be the **past 12** datapoints in the time series, i.e., observation sequence with historical window of 12. You may check the time series data and think about the reason.\n",
        "    - The label Y should be the **next 1** datapoint in the time series (one point ahead prediction).\n",
        "4. The first 3 **TRAIN** data and label should be:\n",
        "\n",
        "- trainX[0] = [[0.02203858 &nbsp; 0.03856748 &nbsp; 0.077135 &nbsp;  0.06887051 &nbsp; 0.04683197 &nbsp; 0.08539945 &nbsp; 0.12121212 &nbsp; 0.12121212 &nbsp; 0.08815429 &nbsp; 0.04132232 &nbsp; 0.    &nbsp; 0.03856748]]\n",
        "- trainY[0] = [0.03030303]\n",
        "\n",
        "- trianX[1] = [[0.03856748 &nbsp; 0.077135 &nbsp;  0.06887051 &nbsp; 0.04683197  &nbsp; 0.08539945  &nbsp; 0.12121212  &nbsp; 0.12121212  &nbsp; 0.08815429  &nbsp; 0.04132232  &nbsp; 0.     &nbsp;  0.03856748   &nbsp; 0.03030303]]\n",
        "- trainY[1] = [0.06060606]\n",
        "\n",
        "- trainX[2] =  [[0.077135 &nbsp;  0.06887051 &nbsp; 0.04683197 &nbsp; 0.08539945 &nbsp; 0.12121212 &nbsp; 0.12121212 &nbsp; 0.08815429 &nbsp; 0.04132232 &nbsp; 0.    &nbsp;     0.03856748 &nbsp; 0.03030303 &nbsp; 0.06060606]]\n",
        "- trainY[2] = [0.10192838]\n",
        "\n",
        "5. Apply the MinMaxScaler to both the train and test data.\\\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html\n",
        "\n",
        "\n",
        "6. After the preparation with scaler fitting, the shapes of trainX, trainY, testX, and testY are as follows:\\\n",
        "trainX.shape = (101, 1, 12)\\\n",
        "trainY.shape = (101,)\\\n",
        "testX.shape = (17, 1, 12)\\\n",
        "testY.shape = (17,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hS7xhrC3-_-1"
      },
      "outputs": [],
      "source": [
        "### Import Libraries ###\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "import tensorflow.keras.layers \n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "### Set random seed to ensure deterministic results\n",
        "import os\n",
        "seed_value = 1\n",
        "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
        "def reset_random_seeds():\n",
        "   tf.random.set_seed(seed_value)\n",
        "   np.random.seed(seed_value)\n",
        "   random.seed(seed_value)\n",
        "reset_random_seeds()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "shAj2Y6IuxUv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32caca9d-b6eb-4eff-9cd7-7fa9168dbd2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainX[0]: [[0.02203858 0.03856748 0.077135   0.06887051 0.04683197 0.08539945\n",
            "  0.12121212 0.12121212 0.08815429 0.04132232 0.         0.03856748]]\n",
            "trainY[0]: [0.03030303]\n",
            "trainX[1]: [[0.03856748 0.077135   0.06887051 0.04683197 0.08539945 0.12121212\n",
            "  0.12121212 0.08815429 0.04132232 0.         0.03856748 0.03030303]]\n",
            "trainY[1]: [0.06060606]\n",
            "trainX[2]: [[0.077135   0.06887051 0.04683197 0.08539945 0.12121212 0.12121212\n",
            "  0.08815429 0.04132232 0.         0.03856748 0.03030303 0.06060606]]\n",
            "trainY[2]: [0.10192838]\n"
          ]
        }
      ],
      "source": [
        "### Prepare and Preprocess Data Here ###\n",
        "from pandas import read_csv\n",
        "\n",
        "### Design a Function to Prepare Observation Sequences and Corresponding Labels ###\n",
        "\n",
        "def create_dataset(dataset, look_back=12):# look_back is used to specify input sequence length\n",
        "  dataX, dataY = [], []\n",
        "  for i in range(len(dataset)-look_back):\n",
        "    dataX.append(dataset[i:i+look_back])\n",
        "    dataY.append(dataset[i+look_back])\n",
        "  return np.array(dataX), np.array(dataY)\n",
        "\n",
        "\n",
        "### Train and Test Data Loading with float32 type ####\n",
        "dataframe_train = read_csv('train.txt', usecols=[1], engine='python') # Read train.txt \n",
        "dataset_train = dataframe_train.values\n",
        "dataset_train = dataset_train.astype('float32') # Specify the data type to 'float32'\n",
        "\n",
        "dataframe_test = read_csv('test.txt', usecols=[1], engine='python') # Read test.txt \n",
        "dataset_test = dataframe_test.values\n",
        "dataset_test = dataset_test.astype('float32') # Specify the data type to 'float32'\n",
        "\n",
        "### Scale Training and Test Data to [0, 1] ###\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler(feature_range=(0, 1)) # specify the scaler\n",
        "train = scaler.fit_transform(dataset_train) # fit the scaler to the training data\n",
        "test = scaler.transform(dataset_test) # fit the scaler to the test data\n",
        "\n",
        "### Training and Test Data Split ###\n",
        "trainX, trainY = create_dataset(train, look_back=12)\n",
        "testX, testY = create_dataset(test, look_back=12)\n",
        "\n",
        "### Training and Test Data Reshape (to fit RNN input) ###\n",
        "trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
        "testX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))\n",
        "\n",
        "# Check Values\n",
        "for i in range(3):\n",
        "  print(\"trainX[\"+str(i)+\"]: \"+ str(trainX[i]))\n",
        "  print(\"trainY[\"+str(i)+\"]: \"+ str(trainY[i]))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Shapes\n",
        "print(trainX.shape)\n",
        "print(trainY.shape)\n",
        "print(testX.shape)\n",
        "print(testY.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dmkTimH022-n",
        "outputId": "d795b617-6698-4847-d862-db54126da453"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(101, 1, 12)\n",
            "(101, 1)\n",
            "(17, 1, 12)\n",
            "(17, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AQ1uHWzX8wlH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5272ff2a-f73d-478b-df3b-0523b10a5821"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[1.0661157  1.1046832  0.8264463  0.70247936 0.5674931  0.64187324\n",
            "   0.70523417 0.6556474  0.8319559  0.8044077  0.87052345 1.013774  ]]\n",
            "\n",
            " [[1.1046832  0.8264463  0.70247936 0.5674931  0.64187324 0.70523417\n",
            "   0.6556474  0.8319559  0.8044077  0.87052345 1.013774   1.2231405 ]]\n",
            "\n",
            " [[0.8264463  0.70247936 0.5674931  0.64187324 0.70523417 0.6556474\n",
            "   0.8319559  0.8044077  0.87052345 1.013774   1.2231405  1.2534435 ]]\n",
            "\n",
            " [[0.70247936 0.5674931  0.64187324 0.70523417 0.6556474  0.8319559\n",
            "   0.8044077  0.87052345 1.013774   1.2231405  1.2534435  0.98898065]]\n",
            "\n",
            " [[0.5674931  0.64187324 0.70523417 0.6556474  0.8319559  0.8044077\n",
            "   0.87052345 1.013774   1.2231405  1.2534435  0.98898065 0.8347107 ]]\n",
            "\n",
            " [[0.64187324 0.70523417 0.6556474  0.8319559  0.8044077  0.87052345\n",
            "   1.013774   1.2231405  1.2534435  0.98898065 0.8347107  0.7107438 ]]\n",
            "\n",
            " [[0.70523417 0.6556474  0.8319559  0.8044077  0.87052345 1.013774\n",
            "   1.2231405  1.2534435  0.98898065 0.8347107  0.7107438  0.8292011 ]]\n",
            "\n",
            " [[0.6556474  0.8319559  0.8044077  0.87052345 1.013774   1.2231405\n",
            "   1.2534435  0.98898065 0.8347107  0.7107438  0.8292011  0.8622589 ]]\n",
            "\n",
            " [[0.8319559  0.8044077  0.87052345 1.013774   1.2231405  1.2534435\n",
            "   0.98898065 0.8347107  0.7107438  0.8292011  0.8622589  0.79063356]]\n",
            "\n",
            " [[0.8044077  0.87052345 1.013774   1.2231405  1.2534435  0.98898065\n",
            "   0.8347107  0.7107438  0.8292011  0.8622589  0.79063356 0.8677685 ]]\n",
            "\n",
            " [[0.87052345 1.013774   1.2231405  1.2534435  0.98898065 0.8347107\n",
            "   0.7107438  0.8292011  0.8622589  0.79063356 0.8677685  0.98347104]]\n",
            "\n",
            " [[1.013774   1.2231405  1.2534435  0.98898065 0.8347107  0.7107438\n",
            "   0.8292011  0.8622589  0.79063356 0.8677685  0.98347104 1.013774  ]]\n",
            "\n",
            " [[1.2231405  1.2534435  0.98898065 0.8347107  0.7107438  0.8292011\n",
            "   0.8622589  0.79063356 0.8677685  0.98347104 1.013774   1.1873279 ]]\n",
            "\n",
            " [[1.2534435  0.98898065 0.8347107  0.7107438  0.8292011  0.8622589\n",
            "   0.79063356 0.8677685  0.98347104 1.013774   1.1873279  1.4269972 ]]\n",
            "\n",
            " [[0.98898065 0.8347107  0.7107438  0.8292011  0.8622589  0.79063356\n",
            "   0.8677685  0.98347104 1.013774   1.1873279  1.4269972  1.3829201 ]]\n",
            "\n",
            " [[0.8347107  0.7107438  0.8292011  0.8622589  0.79063356 0.8677685\n",
            "   0.98347104 1.013774   1.1873279  1.4269972  1.3829201  1.1129477 ]]\n",
            "\n",
            " [[0.7107438  0.8292011  0.8622589  0.79063356 0.8677685  0.98347104\n",
            "   1.013774   1.1873279  1.4269972  1.3829201  1.1129477  0.98347104]]]\n",
            "[[1.2231405 ]\n",
            " [1.2534435 ]\n",
            " [0.98898065]\n",
            " [0.8347107 ]\n",
            " [0.7107438 ]\n",
            " [0.8292011 ]\n",
            " [0.8622589 ]\n",
            " [0.79063356]\n",
            " [0.8677685 ]\n",
            " [0.98347104]\n",
            " [1.013774  ]\n",
            " [1.1873279 ]\n",
            " [1.4269972 ]\n",
            " [1.3829201 ]\n",
            " [1.1129477 ]\n",
            " [0.98347104]\n",
            " [0.78787875]]\n"
          ]
        }
      ],
      "source": [
        "### Print Out the TEST Data and Labels Here ###\n",
        "print(testX)\n",
        "print(testY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0AlakqA_vuFb"
      },
      "source": [
        "### 1.2 - Build the RNN model (20 Points) ##\n",
        "\n",
        "\n",
        "Build an RNN model with SimpleRNN cell. \n",
        "\n",
        "**Tasks:**\n",
        "1. Build an RNN model with 1 simple RNN layer and 1 Dense layer.  **(10 Points)**\n",
        "2. Compile the model. **(5 Points)**\n",
        "3. Train the model for **1000** epochs with **batch_size = 10**. **(5 Points)**\n",
        "\n",
        "**Hints:**  \n",
        "1. You may consider **tensorflow.keras.layers.SimpleRNN(unit_size=4)** to specify RNN cells.\n",
        "2. Use loss function = 'mean_squared_error' and select **Adam** optimizer with **learning_rate=0.005** and other default settings.\n",
        "3. After first epoch, the train loss is changed to around **0.0912**. \n",
        "4. The model summary is as follows:\n",
        "- Total params: 73\n",
        "- Trainable params: 73\n",
        "- Non-trainable params: 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jn92qh8oyq0B"
      },
      "outputs": [],
      "source": [
        "### Build the RNN Model ###\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "\n",
        "keras.backend.clear_session()\n",
        "\n",
        "model = Sequential() # Declare Sequential class and assign it to variable \"model\"\n",
        "model.add(keras.layers.SimpleRNN(units=4)) # Add a simple RNN layer with unit_size=4 in the model \n",
        "model.add(keras.layers.Dense(units=1)) # Add a following Dense layer with units=1 in the model "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GnO-5WT-3hgH"
      },
      "outputs": [],
      "source": [
        "### Compile the RNN Model  ###\n",
        "\n",
        "opt = keras.optimizers.Adam(learning_rate=0.005)\n",
        "model.compile(loss='mean_squared_error', optimizer='adam') # model compiled with mean_squared_error loss and adam optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6tpZAutlzify",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f48024b-bad5-4287-9c91-aa4c2cc964a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "11/11 [==============================] - 3s 4ms/step - loss: 0.1754\n",
            "Epoch 2/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.1121\n",
            "Epoch 3/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0695\n",
            "Epoch 4/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.0433\n",
            "Epoch 5/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0317\n",
            "Epoch 6/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0268\n",
            "Epoch 7/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0246\n",
            "Epoch 8/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.0240\n",
            "Epoch 9/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0235\n",
            "Epoch 10/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0230\n",
            "Epoch 11/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0226\n",
            "Epoch 12/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0218\n",
            "Epoch 13/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.0216\n",
            "Epoch 14/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.0212\n",
            "Epoch 15/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.0206\n",
            "Epoch 16/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.0201\n",
            "Epoch 17/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0197\n",
            "Epoch 18/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.0194\n",
            "Epoch 19/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0191\n",
            "Epoch 20/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.0187\n",
            "Epoch 21/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.0183\n",
            "Epoch 22/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0178\n",
            "Epoch 23/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0174\n",
            "Epoch 24/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0171\n",
            "Epoch 25/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.0167\n",
            "Epoch 26/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0163\n",
            "Epoch 27/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0159\n",
            "Epoch 28/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0157\n",
            "Epoch 29/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.0153\n",
            "Epoch 30/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0148\n",
            "Epoch 31/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0146\n",
            "Epoch 32/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0143\n",
            "Epoch 33/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0139\n",
            "Epoch 34/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0137\n",
            "Epoch 35/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0133\n",
            "Epoch 36/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0130\n",
            "Epoch 37/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0128\n",
            "Epoch 38/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0125\n",
            "Epoch 39/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0125\n",
            "Epoch 40/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0121\n",
            "Epoch 41/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0116\n",
            "Epoch 42/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0114\n",
            "Epoch 43/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0111\n",
            "Epoch 44/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0109\n",
            "Epoch 45/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0108\n",
            "Epoch 46/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0106\n",
            "Epoch 47/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0102\n",
            "Epoch 48/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0100\n",
            "Epoch 49/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0098\n",
            "Epoch 50/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0095\n",
            "Epoch 51/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0095\n",
            "Epoch 52/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0091\n",
            "Epoch 53/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0089\n",
            "Epoch 54/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0087\n",
            "Epoch 55/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0086\n",
            "Epoch 56/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0083\n",
            "Epoch 57/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0081\n",
            "Epoch 58/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0079\n",
            "Epoch 59/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0078\n",
            "Epoch 60/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0076\n",
            "Epoch 61/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0075\n",
            "Epoch 62/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0073\n",
            "Epoch 63/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0071\n",
            "Epoch 64/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0068\n",
            "Epoch 65/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0068\n",
            "Epoch 66/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0066\n",
            "Epoch 67/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0065\n",
            "Epoch 68/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0063\n",
            "Epoch 69/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0061\n",
            "Epoch 70/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0060\n",
            "Epoch 71/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0058\n",
            "Epoch 72/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0057\n",
            "Epoch 73/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0056\n",
            "Epoch 74/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0056\n",
            "Epoch 75/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0054\n",
            "Epoch 76/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0052\n",
            "Epoch 77/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0052\n",
            "Epoch 78/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0050\n",
            "Epoch 79/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0050\n",
            "Epoch 80/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0049\n",
            "Epoch 81/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.0048\n",
            "Epoch 82/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0049\n",
            "Epoch 83/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.0047\n",
            "Epoch 84/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0046\n",
            "Epoch 85/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.0046\n",
            "Epoch 86/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.0043\n",
            "Epoch 87/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.0043\n",
            "Epoch 88/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.0043\n",
            "Epoch 89/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0041\n",
            "Epoch 90/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0040\n",
            "Epoch 91/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.0040\n",
            "Epoch 92/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0040\n",
            "Epoch 93/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.0038\n",
            "Epoch 94/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.0038\n",
            "Epoch 95/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.0037\n",
            "Epoch 96/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.0036\n",
            "Epoch 97/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0036\n",
            "Epoch 98/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0036\n",
            "Epoch 99/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0035\n",
            "Epoch 100/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.0035\n",
            "Epoch 101/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.0034\n",
            "Epoch 102/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.0034\n",
            "Epoch 103/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.0034\n",
            "Epoch 104/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0033\n",
            "Epoch 105/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.0033\n",
            "Epoch 106/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0032\n",
            "Epoch 107/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.0032\n",
            "Epoch 108/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0031\n",
            "Epoch 109/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0031\n",
            "Epoch 110/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0031\n",
            "Epoch 111/1000\n",
            "11/11 [==============================] - 0s 10ms/step - loss: 0.0030\n",
            "Epoch 112/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0030\n",
            "Epoch 113/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0029\n",
            "Epoch 114/1000\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.0029\n",
            "Epoch 115/1000\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.0029\n",
            "Epoch 116/1000\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.0029\n",
            "Epoch 117/1000\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.0028\n",
            "Epoch 118/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0028\n",
            "Epoch 119/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0028\n",
            "Epoch 120/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.0027\n",
            "Epoch 121/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.0028\n",
            "Epoch 122/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0027\n",
            "Epoch 123/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.0027\n",
            "Epoch 124/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0027\n",
            "Epoch 125/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0026\n",
            "Epoch 126/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.0026\n",
            "Epoch 127/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0026\n",
            "Epoch 128/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0026\n",
            "Epoch 129/1000\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.0025\n",
            "Epoch 130/1000\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.0025\n",
            "Epoch 131/1000\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.0024\n",
            "Epoch 132/1000\n",
            "11/11 [==============================] - 0s 9ms/step - loss: 0.0025\n",
            "Epoch 133/1000\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.0024\n",
            "Epoch 134/1000\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.0024\n",
            "Epoch 135/1000\n",
            "11/11 [==============================] - 0s 9ms/step - loss: 0.0024\n",
            "Epoch 136/1000\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.0024\n",
            "Epoch 137/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0024\n",
            "Epoch 138/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.0023\n",
            "Epoch 139/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0023\n",
            "Epoch 140/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.0023\n",
            "Epoch 141/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0023\n",
            "Epoch 142/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0024\n",
            "Epoch 143/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0022\n",
            "Epoch 144/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0022\n",
            "Epoch 145/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0022\n",
            "Epoch 146/1000\n",
            "11/11 [==============================] - 0s 9ms/step - loss: 0.0022\n",
            "Epoch 147/1000\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.0022\n",
            "Epoch 148/1000\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.0022\n",
            "Epoch 149/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0022\n",
            "Epoch 150/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0022\n",
            "Epoch 151/1000\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.0021\n",
            "Epoch 152/1000\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.0021\n",
            "Epoch 153/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0021\n",
            "Epoch 154/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0021\n",
            "Epoch 155/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0021\n",
            "Epoch 156/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0020\n",
            "Epoch 157/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0020\n",
            "Epoch 158/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0020\n",
            "Epoch 159/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.0020\n",
            "Epoch 160/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.0020\n",
            "Epoch 161/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0020\n",
            "Epoch 162/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0021\n",
            "Epoch 163/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0020\n",
            "Epoch 164/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0019\n",
            "Epoch 165/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.0019\n",
            "Epoch 166/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0019\n",
            "Epoch 167/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0019\n",
            "Epoch 168/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0019\n",
            "Epoch 169/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0019\n",
            "Epoch 170/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0019\n",
            "Epoch 171/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0019\n",
            "Epoch 172/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.0021\n",
            "Epoch 173/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.0021\n",
            "Epoch 174/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0019\n",
            "Epoch 175/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0018\n",
            "Epoch 176/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0018\n",
            "Epoch 177/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.0018\n",
            "Epoch 178/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0018\n",
            "Epoch 179/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0018\n",
            "Epoch 180/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0018\n",
            "Epoch 181/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0017\n",
            "Epoch 182/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.0018\n",
            "Epoch 183/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0017\n",
            "Epoch 184/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.0017\n",
            "Epoch 185/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0017\n",
            "Epoch 186/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0017\n",
            "Epoch 187/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0017\n",
            "Epoch 188/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.0017\n",
            "Epoch 189/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0017\n",
            "Epoch 190/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0017\n",
            "Epoch 191/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.0017\n",
            "Epoch 192/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0017\n",
            "Epoch 193/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0017\n",
            "Epoch 194/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.0016\n",
            "Epoch 195/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0017\n",
            "Epoch 196/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.0016\n",
            "Epoch 197/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0017\n",
            "Epoch 198/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0016\n",
            "Epoch 199/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0016\n",
            "Epoch 200/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0017\n",
            "Epoch 201/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0018\n",
            "Epoch 202/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0016\n",
            "Epoch 203/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0016\n",
            "Epoch 204/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0016\n",
            "Epoch 205/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0016\n",
            "Epoch 206/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0016\n",
            "Epoch 207/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0015\n",
            "Epoch 208/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0015\n",
            "Epoch 209/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0016\n",
            "Epoch 210/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0015\n",
            "Epoch 211/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0017\n",
            "Epoch 212/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0015\n",
            "Epoch 213/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0015\n",
            "Epoch 214/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0015\n",
            "Epoch 215/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0015\n",
            "Epoch 216/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0016\n",
            "Epoch 217/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0016\n",
            "Epoch 218/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0015\n",
            "Epoch 219/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0015\n",
            "Epoch 220/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0016\n",
            "Epoch 221/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0015\n",
            "Epoch 222/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0015\n",
            "Epoch 223/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0015\n",
            "Epoch 224/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0015\n",
            "Epoch 225/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0015\n",
            "Epoch 226/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.0015\n",
            "Epoch 227/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0015\n",
            "Epoch 228/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0015\n",
            "Epoch 229/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0015\n",
            "Epoch 230/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0015\n",
            "Epoch 231/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0014\n",
            "Epoch 232/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0015\n",
            "Epoch 233/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.0014\n",
            "Epoch 234/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0014\n",
            "Epoch 235/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.0014\n",
            "Epoch 236/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0014\n",
            "Epoch 237/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0014\n",
            "Epoch 238/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0014\n",
            "Epoch 239/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0015\n",
            "Epoch 240/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0014\n",
            "Epoch 241/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.0015\n",
            "Epoch 242/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0014\n",
            "Epoch 243/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0014\n",
            "Epoch 244/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0014\n",
            "Epoch 245/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.0014\n",
            "Epoch 246/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0014\n",
            "Epoch 247/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0014\n",
            "Epoch 248/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.0014\n",
            "Epoch 249/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0014\n",
            "Epoch 250/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.0014\n",
            "Epoch 251/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0014\n",
            "Epoch 252/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0014\n",
            "Epoch 253/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0014\n",
            "Epoch 254/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.0014\n",
            "Epoch 255/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0015\n",
            "Epoch 256/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0014\n",
            "Epoch 257/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.0014\n",
            "Epoch 258/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0013\n",
            "Epoch 259/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0014\n",
            "Epoch 260/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.0013\n",
            "Epoch 261/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0014\n",
            "Epoch 262/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0014\n",
            "Epoch 263/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0013\n",
            "Epoch 264/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0013\n",
            "Epoch 265/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0014\n",
            "Epoch 266/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0013\n",
            "Epoch 267/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0014\n",
            "Epoch 268/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0015\n",
            "Epoch 269/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0014\n",
            "Epoch 270/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.0013\n",
            "Epoch 271/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.0014\n",
            "Epoch 272/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.0013\n",
            "Epoch 273/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0014\n",
            "Epoch 274/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.0013\n",
            "Epoch 275/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0013\n",
            "Epoch 276/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.0013\n",
            "Epoch 277/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.0013\n",
            "Epoch 278/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.0013\n",
            "Epoch 279/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0013\n",
            "Epoch 280/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0013\n",
            "Epoch 281/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.0013\n",
            "Epoch 282/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0014\n",
            "Epoch 283/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0013\n",
            "Epoch 284/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0013\n",
            "Epoch 285/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.0013\n",
            "Epoch 286/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0013\n",
            "Epoch 287/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0013\n",
            "Epoch 288/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0013\n",
            "Epoch 289/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.0014\n",
            "Epoch 290/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.0014\n",
            "Epoch 291/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0013\n",
            "Epoch 292/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.0013\n",
            "Epoch 293/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0013\n",
            "Epoch 294/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.0013\n",
            "Epoch 295/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0013\n",
            "Epoch 296/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.0013\n",
            "Epoch 297/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0013\n",
            "Epoch 298/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.0013\n",
            "Epoch 299/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.0014\n",
            "Epoch 300/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.0012\n",
            "Epoch 301/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0013\n",
            "Epoch 302/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.0013\n",
            "Epoch 303/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0013\n",
            "Epoch 304/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0012\n",
            "Epoch 305/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.0013\n",
            "Epoch 306/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.0013\n",
            "Epoch 307/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.0013\n",
            "Epoch 308/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0013\n",
            "Epoch 309/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0013\n",
            "Epoch 310/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0013\n",
            "Epoch 311/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0012\n",
            "Epoch 312/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.0013\n",
            "Epoch 313/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0012\n",
            "Epoch 314/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0013\n",
            "Epoch 315/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0012\n",
            "Epoch 316/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0013\n",
            "Epoch 317/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0012\n",
            "Epoch 318/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.0012\n",
            "Epoch 319/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0012\n",
            "Epoch 320/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.0012\n",
            "Epoch 321/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0014\n",
            "Epoch 322/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.0013\n",
            "Epoch 323/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0012\n",
            "Epoch 324/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0012\n",
            "Epoch 325/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.0012\n",
            "Epoch 326/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0012\n",
            "Epoch 327/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0012\n",
            "Epoch 328/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0013\n",
            "Epoch 329/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0012\n",
            "Epoch 330/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0012\n",
            "Epoch 331/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.0012\n",
            "Epoch 332/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0013\n",
            "Epoch 333/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.0012\n",
            "Epoch 334/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0013\n",
            "Epoch 335/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.0014\n",
            "Epoch 336/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0012\n",
            "Epoch 337/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0013\n",
            "Epoch 338/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.0013\n",
            "Epoch 339/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0012\n",
            "Epoch 340/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0013\n",
            "Epoch 341/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0012\n",
            "Epoch 342/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0012\n",
            "Epoch 343/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0012\n",
            "Epoch 344/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0013\n",
            "Epoch 345/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0013\n",
            "Epoch 346/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.0012\n",
            "Epoch 347/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0012\n",
            "Epoch 348/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0012\n",
            "Epoch 349/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0012\n",
            "Epoch 350/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0014\n",
            "Epoch 351/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0012\n",
            "Epoch 352/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.0012\n",
            "Epoch 353/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0013\n",
            "Epoch 354/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.0013\n",
            "Epoch 355/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.0012\n",
            "Epoch 356/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0012\n",
            "Epoch 357/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0012\n",
            "Epoch 358/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0012\n",
            "Epoch 359/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0012\n",
            "Epoch 360/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0012\n",
            "Epoch 361/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.0012\n",
            "Epoch 362/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0012\n",
            "Epoch 363/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0012\n",
            "Epoch 364/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0012\n",
            "Epoch 365/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0012\n",
            "Epoch 366/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0012\n",
            "Epoch 367/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0012\n",
            "Epoch 368/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0013\n",
            "Epoch 369/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0012\n",
            "Epoch 370/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0012\n",
            "Epoch 371/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0013\n",
            "Epoch 372/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0012\n",
            "Epoch 373/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 374/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0012\n",
            "Epoch 375/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0012\n",
            "Epoch 376/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0012\n",
            "Epoch 377/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0012\n",
            "Epoch 378/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.0013\n",
            "Epoch 379/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0012\n",
            "Epoch 380/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0012\n",
            "Epoch 381/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0012\n",
            "Epoch 382/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0012\n",
            "Epoch 383/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0012\n",
            "Epoch 384/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.0012\n",
            "Epoch 385/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.0011\n",
            "Epoch 386/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0012\n",
            "Epoch 387/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0012\n",
            "Epoch 388/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0012\n",
            "Epoch 389/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0013\n",
            "Epoch 390/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0012\n",
            "Epoch 391/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0012\n",
            "Epoch 392/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 393/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.0011\n",
            "Epoch 394/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.0011\n",
            "Epoch 395/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0012\n",
            "Epoch 396/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0012\n",
            "Epoch 397/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 398/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.0012\n",
            "Epoch 399/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0012\n",
            "Epoch 400/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.0011\n",
            "Epoch 401/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 402/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.0011\n",
            "Epoch 403/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.0012\n",
            "Epoch 404/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0011\n",
            "Epoch 405/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0012\n",
            "Epoch 406/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 407/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.0011\n",
            "Epoch 408/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.0011\n",
            "Epoch 409/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0013\n",
            "Epoch 410/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0012\n",
            "Epoch 411/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0012\n",
            "Epoch 412/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0014\n",
            "Epoch 413/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0014\n",
            "Epoch 414/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 415/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0013\n",
            "Epoch 416/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.0011\n",
            "Epoch 417/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.0012\n",
            "Epoch 418/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0012\n",
            "Epoch 419/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 420/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0012\n",
            "Epoch 421/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.0012\n",
            "Epoch 422/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0012\n",
            "Epoch 423/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.0011\n",
            "Epoch 424/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 425/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 426/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 427/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0012\n",
            "Epoch 428/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0011\n",
            "Epoch 429/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.0012\n",
            "Epoch 430/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 431/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 432/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0013\n",
            "Epoch 433/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0012\n",
            "Epoch 434/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 435/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.0012\n",
            "Epoch 436/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0012\n",
            "Epoch 437/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 438/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 439/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 440/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 441/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.0011\n",
            "Epoch 442/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 443/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.0011\n",
            "Epoch 444/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.0011\n",
            "Epoch 445/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 446/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 447/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0012\n",
            "Epoch 448/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0011\n",
            "Epoch 449/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 450/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 451/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 452/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 453/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 454/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 455/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.0011\n",
            "Epoch 456/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0012\n",
            "Epoch 457/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0012\n",
            "Epoch 458/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 459/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 460/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 461/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 462/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 463/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 464/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 465/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 466/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0012\n",
            "Epoch 467/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0011\n",
            "Epoch 468/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 469/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 470/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 471/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 472/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 473/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 474/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0012\n",
            "Epoch 475/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 476/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0012\n",
            "Epoch 477/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 478/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.0011\n",
            "Epoch 479/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 480/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 481/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0013\n",
            "Epoch 482/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0012\n",
            "Epoch 483/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0012\n",
            "Epoch 484/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 485/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 486/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0011\n",
            "Epoch 487/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0012\n",
            "Epoch 488/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0012\n",
            "Epoch 489/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 490/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 491/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 492/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 493/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 494/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 495/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0012\n",
            "Epoch 496/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 497/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 498/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 499/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0013\n",
            "Epoch 500/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 501/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 502/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 503/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 504/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 505/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 506/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 507/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0012\n",
            "Epoch 508/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 509/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 510/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.0014\n",
            "Epoch 511/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0013\n",
            "Epoch 512/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 513/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 514/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0010\n",
            "Epoch 515/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 516/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 517/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0012\n",
            "Epoch 518/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 519/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 520/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 521/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 522/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 523/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 524/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 525/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 526/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0012\n",
            "Epoch 527/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 528/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 529/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 530/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 531/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 532/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0012\n",
            "Epoch 533/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 534/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 535/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0013\n",
            "Epoch 536/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0014\n",
            "Epoch 537/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.0012\n",
            "Epoch 538/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 539/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.0011\n",
            "Epoch 540/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 541/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0010\n",
            "Epoch 542/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0012\n",
            "Epoch 543/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 544/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 545/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 546/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0010\n",
            "Epoch 547/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 548/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 549/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 550/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 551/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 552/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 553/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 554/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 555/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 556/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0011\n",
            "Epoch 557/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 558/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0012\n",
            "Epoch 559/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0013\n",
            "Epoch 560/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 561/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0012\n",
            "Epoch 562/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 563/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 564/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 565/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 566/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 567/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 568/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 569/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 570/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 571/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 572/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0013\n",
            "Epoch 573/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 574/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 575/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.0011\n",
            "Epoch 576/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0010\n",
            "Epoch 577/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0010\n",
            "Epoch 578/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0011\n",
            "Epoch 579/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 580/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0010\n",
            "Epoch 581/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 582/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 583/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 584/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 585/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 586/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 587/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 588/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 589/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0010\n",
            "Epoch 590/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 591/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 592/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 593/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 594/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 595/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0010\n",
            "Epoch 596/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 597/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 598/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 599/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 600/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 601/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 602/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 603/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 604/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 605/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 606/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 607/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 608/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 609/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 610/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 611/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 612/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 613/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0010\n",
            "Epoch 614/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 615/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.8034e-04\n",
            "Epoch 616/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 617/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 618/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 619/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 620/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 621/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 622/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 623/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 624/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 625/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 626/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0010\n",
            "Epoch 627/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 628/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 629/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 630/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 631/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0010\n",
            "Epoch 632/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 633/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.9775e-04\n",
            "Epoch 634/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 635/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0012\n",
            "Epoch 636/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.9992e-04\n",
            "Epoch 637/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 638/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 639/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 640/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 641/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.9263e-04\n",
            "Epoch 642/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 643/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 644/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 645/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 646/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.7487e-04\n",
            "Epoch 647/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.9633e-04\n",
            "Epoch 648/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.8808e-04\n",
            "Epoch 649/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0012\n",
            "Epoch 650/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0012\n",
            "Epoch 651/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 652/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 653/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 654/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 655/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 656/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 657/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 658/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 659/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 660/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.7245e-04\n",
            "Epoch 661/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 662/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.8960e-04\n",
            "Epoch 663/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.9013e-04\n",
            "Epoch 664/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 665/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 666/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.9253e-04\n",
            "Epoch 667/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 668/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 669/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.8596e-04\n",
            "Epoch 670/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 671/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 672/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.9299e-04\n",
            "Epoch 673/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 674/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.9716e-04\n",
            "Epoch 675/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 676/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 677/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 678/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 679/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 680/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 681/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0011\n",
            "Epoch 682/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 683/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 684/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.9851e-04\n",
            "Epoch 685/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.8619e-04\n",
            "Epoch 686/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 687/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 688/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 689/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.7566e-04\n",
            "Epoch 690/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 691/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.8072e-04\n",
            "Epoch 692/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 693/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 694/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0013\n",
            "Epoch 695/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 696/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 697/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 698/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 699/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.9416e-04\n",
            "Epoch 700/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0010\n",
            "Epoch 701/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.7986e-04\n",
            "Epoch 702/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 9.8860e-04\n",
            "Epoch 703/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.9580e-04\n",
            "Epoch 704/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 9.7624e-04\n",
            "Epoch 705/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.9562e-04\n",
            "Epoch 706/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.9606e-04\n",
            "Epoch 707/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0010\n",
            "Epoch 708/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 709/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 710/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.7979e-04\n",
            "Epoch 711/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 712/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 713/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 714/1000\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 9.9297e-04\n",
            "Epoch 715/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.9640e-04\n",
            "Epoch 716/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.9770e-04\n",
            "Epoch 717/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.8507e-04\n",
            "Epoch 718/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 719/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 720/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 9.7291e-04\n",
            "Epoch 721/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.8761e-04\n",
            "Epoch 722/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 723/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.8339e-04\n",
            "Epoch 724/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.9459e-04\n",
            "Epoch 725/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 726/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 727/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.7461e-04\n",
            "Epoch 728/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 729/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 730/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.7493e-04\n",
            "Epoch 731/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 732/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 733/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 9.6942e-04\n",
            "Epoch 734/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 9.7017e-04\n",
            "Epoch 735/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.8097e-04\n",
            "Epoch 736/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.9289e-04\n",
            "Epoch 737/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0010\n",
            "Epoch 738/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 9.7788e-04\n",
            "Epoch 739/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0010\n",
            "Epoch 740/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 9.6616e-04\n",
            "Epoch 741/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0011\n",
            "Epoch 742/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0011\n",
            "Epoch 743/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 744/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 745/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0011\n",
            "Epoch 746/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 747/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 9.9571e-04\n",
            "Epoch 748/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 9.8448e-04\n",
            "Epoch 749/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0010\n",
            "Epoch 750/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.9259e-04\n",
            "Epoch 751/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.6440e-04\n",
            "Epoch 752/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 9.7614e-04\n",
            "Epoch 753/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 9.6923e-04\n",
            "Epoch 754/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.7538e-04\n",
            "Epoch 755/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.9156e-04\n",
            "Epoch 756/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0010\n",
            "Epoch 757/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0012\n",
            "Epoch 758/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0014\n",
            "Epoch 759/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0013\n",
            "Epoch 760/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 761/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.9513e-04\n",
            "Epoch 762/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0010\n",
            "Epoch 763/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 9.7166e-04\n",
            "Epoch 764/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 9.7004e-04\n",
            "Epoch 765/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 9.8616e-04\n",
            "Epoch 766/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 9.7818e-04\n",
            "Epoch 767/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 9.7474e-04\n",
            "Epoch 768/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 9.7922e-04\n",
            "Epoch 769/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0010\n",
            "Epoch 770/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0011\n",
            "Epoch 771/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0010\n",
            "Epoch 772/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 9.8542e-04\n",
            "Epoch 773/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 9.9623e-04\n",
            "Epoch 774/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0010\n",
            "Epoch 775/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0011\n",
            "Epoch 776/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0012\n",
            "Epoch 777/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0012\n",
            "Epoch 778/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 9.6708e-04\n",
            "Epoch 779/1000\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 9.9758e-04\n",
            "Epoch 780/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 9.8337e-04\n",
            "Epoch 781/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 9.9721e-04\n",
            "Epoch 782/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 9.7326e-04\n",
            "Epoch 783/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 9.9239e-04\n",
            "Epoch 784/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 9.6735e-04\n",
            "Epoch 785/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 9.6704e-04\n",
            "Epoch 786/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 9.9506e-04\n",
            "Epoch 787/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0010\n",
            "Epoch 788/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 9.8740e-04\n",
            "Epoch 789/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 9.6278e-04\n",
            "Epoch 790/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 9.7644e-04\n",
            "Epoch 791/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 9.5962e-04\n",
            "Epoch 792/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 9.6755e-04\n",
            "Epoch 793/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0011\n",
            "Epoch 794/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0011\n",
            "Epoch 795/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 9.6417e-04\n",
            "Epoch 796/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0010\n",
            "Epoch 797/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 9.7034e-04\n",
            "Epoch 798/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 9.8346e-04\n",
            "Epoch 799/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 9.7853e-04\n",
            "Epoch 800/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 9.7710e-04\n",
            "Epoch 801/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 9.8059e-04\n",
            "Epoch 802/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 9.5904e-04\n",
            "Epoch 803/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0012\n",
            "Epoch 804/1000\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0011\n",
            "Epoch 805/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 9.8580e-04\n",
            "Epoch 806/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0011\n",
            "Epoch 807/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0010\n",
            "Epoch 808/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 9.9493e-04\n",
            "Epoch 809/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.6211e-04\n",
            "Epoch 810/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 9.6582e-04\n",
            "Epoch 811/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 9.7023e-04\n",
            "Epoch 812/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 9.6668e-04\n",
            "Epoch 813/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.5158e-04\n",
            "Epoch 814/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.8115e-04\n",
            "Epoch 815/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.8101e-04\n",
            "Epoch 816/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 817/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.8709e-04\n",
            "Epoch 818/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 9.6471e-04\n",
            "Epoch 819/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 9.5769e-04\n",
            "Epoch 820/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.6198e-04\n",
            "Epoch 821/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.6128e-04\n",
            "Epoch 822/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.5440e-04\n",
            "Epoch 823/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 824/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 9.7200e-04\n",
            "Epoch 825/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 826/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.8592e-04\n",
            "Epoch 827/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.6126e-04\n",
            "Epoch 828/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.8046e-04\n",
            "Epoch 829/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 830/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.9096e-04\n",
            "Epoch 831/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.9955e-04\n",
            "Epoch 832/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.7404e-04\n",
            "Epoch 833/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.6452e-04\n",
            "Epoch 834/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.8955e-04\n",
            "Epoch 835/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 836/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0012\n",
            "Epoch 837/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0011\n",
            "Epoch 838/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.9614e-04\n",
            "Epoch 839/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.9123e-04\n",
            "Epoch 840/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.9089e-04\n",
            "Epoch 841/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.4928e-04\n",
            "Epoch 842/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 9.7750e-04\n",
            "Epoch 843/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.8770e-04\n",
            "Epoch 844/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0012\n",
            "Epoch 845/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0015\n",
            "Epoch 846/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 847/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.6700e-04\n",
            "Epoch 848/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0010\n",
            "Epoch 849/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0014\n",
            "Epoch 850/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 851/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 852/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 853/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.9927e-04\n",
            "Epoch 854/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 855/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 856/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 9.6436e-04\n",
            "Epoch 857/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.8122e-04\n",
            "Epoch 858/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.9309e-04\n",
            "Epoch 859/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.9846e-04\n",
            "Epoch 860/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 861/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.9472e-04\n",
            "Epoch 862/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0010\n",
            "Epoch 863/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.8424e-04\n",
            "Epoch 864/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.6950e-04\n",
            "Epoch 865/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.6207e-04\n",
            "Epoch 866/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0012\n",
            "Epoch 867/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 868/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 869/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.5043e-04\n",
            "Epoch 870/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.8985e-04\n",
            "Epoch 871/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.6641e-04\n",
            "Epoch 872/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 873/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0011\n",
            "Epoch 874/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.5390e-04\n",
            "Epoch 875/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.6330e-04\n",
            "Epoch 876/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.6955e-04\n",
            "Epoch 877/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.9433e-04\n",
            "Epoch 878/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.6041e-04\n",
            "Epoch 879/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.9003e-04\n",
            "Epoch 880/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.6611e-04\n",
            "Epoch 881/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.7048e-04\n",
            "Epoch 882/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.7569e-04\n",
            "Epoch 883/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 884/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.9437e-04\n",
            "Epoch 885/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.7743e-04\n",
            "Epoch 886/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.6382e-04\n",
            "Epoch 887/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 888/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.7927e-04\n",
            "Epoch 889/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0010\n",
            "Epoch 890/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0012\n",
            "Epoch 891/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0010\n",
            "Epoch 892/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 9.5494e-04\n",
            "Epoch 893/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 9.6554e-04\n",
            "Epoch 894/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.7959e-04\n",
            "Epoch 895/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.9793e-04\n",
            "Epoch 896/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 897/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0011\n",
            "Epoch 898/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 899/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 900/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 9.6084e-04\n",
            "Epoch 901/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.4633e-04\n",
            "Epoch 902/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.5850e-04\n",
            "Epoch 903/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.5021e-04\n",
            "Epoch 904/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 905/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 9.4850e-04\n",
            "Epoch 906/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.5940e-04\n",
            "Epoch 907/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.4216e-04\n",
            "Epoch 908/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 9.4666e-04\n",
            "Epoch 909/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 9.4998e-04\n",
            "Epoch 910/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.5692e-04\n",
            "Epoch 911/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.6802e-04\n",
            "Epoch 912/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 913/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.6929e-04\n",
            "Epoch 914/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0012\n",
            "Epoch 915/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 916/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 917/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 918/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.4625e-04\n",
            "Epoch 919/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.6663e-04\n",
            "Epoch 920/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.4443e-04\n",
            "Epoch 921/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.9114e-04\n",
            "Epoch 922/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.5021e-04\n",
            "Epoch 923/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.4354e-04\n",
            "Epoch 924/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 925/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 9.4582e-04\n",
            "Epoch 926/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 9.4784e-04\n",
            "Epoch 927/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.6674e-04\n",
            "Epoch 928/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.7443e-04\n",
            "Epoch 929/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.5373e-04\n",
            "Epoch 930/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 931/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 9.5109e-04\n",
            "Epoch 932/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.4534e-04\n",
            "Epoch 933/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.4243e-04\n",
            "Epoch 934/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.6315e-04\n",
            "Epoch 935/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 936/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 937/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.3424e-04\n",
            "Epoch 938/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 939/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.9399e-04\n",
            "Epoch 940/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 941/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.9138e-04\n",
            "Epoch 942/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 943/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.5197e-04\n",
            "Epoch 944/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 9.5846e-04\n",
            "Epoch 945/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 9.7658e-04\n",
            "Epoch 946/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.6296e-04\n",
            "Epoch 947/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.9297e-04\n",
            "Epoch 948/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.4606e-04\n",
            "Epoch 949/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.6595e-04\n",
            "Epoch 950/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 951/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 952/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.5959e-04\n",
            "Epoch 953/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.6657e-04\n",
            "Epoch 954/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.6642e-04\n",
            "Epoch 955/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.6070e-04\n",
            "Epoch 956/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.4114e-04\n",
            "Epoch 957/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.5426e-04\n",
            "Epoch 958/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.5566e-04\n",
            "Epoch 959/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0014\n",
            "Epoch 960/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0013\n",
            "Epoch 961/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0013\n",
            "Epoch 962/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0010\n",
            "Epoch 963/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.5761e-04\n",
            "Epoch 964/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0011\n",
            "Epoch 965/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.9799e-04\n",
            "Epoch 966/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 9.4722e-04\n",
            "Epoch 967/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.9045e-04\n",
            "Epoch 968/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.8109e-04\n",
            "Epoch 969/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 970/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.9489e-04\n",
            "Epoch 971/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 9.5633e-04\n",
            "Epoch 972/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0011\n",
            "Epoch 973/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.6646e-04\n",
            "Epoch 974/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.7739e-04\n",
            "Epoch 975/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 976/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.5711e-04\n",
            "Epoch 977/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.6627e-04\n",
            "Epoch 978/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.6789e-04\n",
            "Epoch 979/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 980/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.4799e-04\n",
            "Epoch 981/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 9.4507e-04\n",
            "Epoch 982/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.5236e-04\n",
            "Epoch 983/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.4131e-04\n",
            "Epoch 984/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0010\n",
            "Epoch 985/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 986/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 9.8261e-04\n",
            "Epoch 987/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.4148e-04\n",
            "Epoch 988/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.8310e-04\n",
            "Epoch 989/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.7086e-04\n",
            "Epoch 990/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 9.4246e-04\n",
            "Epoch 991/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.4609e-04\n",
            "Epoch 992/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.6748e-04\n",
            "Epoch 993/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.4965e-04\n",
            "Epoch 994/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0010\n",
            "Epoch 995/1000\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 9.7032e-04\n",
            "Epoch 996/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011\n",
            "Epoch 997/1000\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 9.9928e-04\n",
            "Epoch 998/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.4567e-04\n",
            "Epoch 999/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.5828e-04\n",
            "Epoch 1000/1000\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.5735e-04\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " simple_rnn (SimpleRNN)      (None, 4)                 68        \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 5         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 73\n",
            "Trainable params: 73\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "### Train the RNN Model  ###\n",
        "\n",
        "model.fit(trainX, trainY, epochs=1000, batch_size=10) # model fit with epoch=1000, batch_size=10; verbose=2 is optional.\n",
        "model.summary() # print out model structure with model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dd2jZl4n0H8m"
      },
      "source": [
        "### 1.3 Evaluate Predictive Model Performance (10 Points)\n",
        "\n",
        "Predict datapoints with the observed datapoints and trained model. \n",
        "\n",
        "**Tasks:**\n",
        "1. Do direct prediction on train and test datapoints with the obtained model in section 1.2. **(2 Points)**\n",
        "2. Scale the prediction results back to original representation with the scaler.(scaler.inverse_transform function) **(3 Points)**\n",
        "3. Calculate root mean squared error (RMSE) and **print out** the error for **both TRAIN and TEST**. **(3 Points)**\n",
        "4. **Plot** the **TEST** label and prediction. **(2 Points)**\n",
        "\n",
        "\n",
        "**Hints:**  \n",
        "1. Scale back the predictions with the build-in function \"scaler.inverse_transform\".\\\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html#sklearn.preprocessing.MinMaxScaler.inverse_transform\n",
        "2. For validation: Train Score: **~10.92 RMSE** Test Score: **~27.70 RMSE**\n",
        "3. The plot for validation is shown below (observation test data are blue and prediction results are orange):\n",
        "\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=10c1Fsa_9v0AQf2fDpCzGPFPxbIEso81u)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KNEkAMxnz8Mq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "416225ad-5a43-45d2-a6e6-9b3f19ee4d0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4/4 [==============================] - 0s 3ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n"
          ]
        }
      ],
      "source": [
        "### Make Predictions ###\n",
        "\n",
        "trainPredict = model.predict(trainX)\n",
        "testPredict = model.predict(testX)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LbnRqEv9z-he"
      },
      "outputs": [],
      "source": [
        "### Scale Back Predictions ###\n",
        "\n",
        "trainPredict = scaler.inverse_transform(trainPredict) # scale train prediction back with scaler.inverse_transform()\n",
        "trainY = scaler.inverse_transform(trainY)  # scale train labels back with scaler.inverse_transform()\n",
        "\n",
        "testPredict = scaler.inverse_transform(testPredict) # scale test prediction back with scaler.inverse_transform()\n",
        "testY = scaler.inverse_transform(testY) # scale test labels back with scaler.inverse_transform()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xdBWzmE91G6_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "221739f4-4c73-45b0-b321-eca43d6a146d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Score: 11.18 RMSE\n",
            "Test Score: 28.98 RMSE\n"
          ]
        }
      ],
      "source": [
        "### Calculate Root Mean Squared Error (RMSE) ###\n",
        "import math\n",
        "from sklearn.metrics import mean_squared_error # Import mean_squared_error from sklearn.metrics\n",
        "\n",
        "trainScore = math.sqrt(mean_squared_error(trainY, trainPredict[:,0])) \n",
        "testScore = math.sqrt(mean_squared_error(testY, testPredict[:,0])) \n",
        "\n",
        "print('Train Score: %.2f RMSE' % (trainScore))\n",
        "print('Test Score: %.2f RMSE' % (testScore))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "txdu8q7l1aju",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "c35e993c-a328-48ba-e5f3-093dc060c7a1"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "### Plot Observation Data and Prediction Results with TEST dataset ###\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(testY) # Plot Observations in Test Set\n",
        "plt.plot(testPredict) # Plot Predictions in Test Set\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O49Ug-FhtCg8"
      },
      "source": [
        "## 2 - Build an LSTM model to conduct sentiment analysis ##\n",
        "\n",
        "### 2.1 Prepare the data (13 Points) ###\n",
        "\n",
        "Prepare IMDB data for reccurent neural network training.\n",
        "\n",
        "**Tasks:**\n",
        "1. Load the data from IMDB review dataset and **print out** the lengths of sequences. **(3 Points)**\n",
        "2. Preprocess review data to meet the network input requirement by specifying **number of words=1000**, setting **the analysis length of the review = 100**, and **padding the input sequences**. **(10 Points)**\n",
        "\n",
        "**Hints:**  \n",
        "1. You may load the IMDB data with keras.datasets.imdb.load_data(num_words=max_features). Here. max_features is set to **1000**.\n",
        "2. You may use keras.preprocessing.sequence.pad_sequences(x_train, maxlen) to pad the input sequences and set maxlen to **100**.\n",
        "\n",
        "**Note:**\\\n",
        "We train the built LSTM-based model with ALL training data; the **validation set** (aka **development set**) is set with the **testing set** for model evaluation. This split is common in the application with limited sampled observation data, like NLP problems.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UI4ki461S2V3"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "from keras import layers\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "### Set random seed to ensure deterministic results\n",
        "import os\n",
        "seed_value = 1\n",
        "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
        "def reset_random_seeds():\n",
        "   tf.random.set_seed(seed_value)\n",
        "   np.random.seed(seed_value)\n",
        "   random.seed(seed_value)\n",
        "\n",
        "reset_random_seeds() # randomly set initial data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LvV1Sv2a18SM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69b0ea8c-ba8c-4a96-b5c2-39f9c9f2e9ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17464789/17464789 [==============================] - 0s 0us/step\n",
            "25000 Training sequences\n",
            "25000 Validation sequences\n"
          ]
        }
      ],
      "source": [
        "# Prepare the data here\n",
        "\n",
        "max_features = 1000 # Only consider the top 1k words\n",
        "maxlen =  100 # Only consider the first 100 words of each movie review\n",
        "\n",
        "(x_train, y_train), (x_val, y_val) = keras.datasets.imdb.load_data() # load IMDB data with specified num_words = 1000; testing set is set to validation set.\n",
        "print(len(x_train), \"Training sequences\")\n",
        "print(len(x_val), \"Validation sequences\")\n",
        "x_train = keras.preprocessing.sequence.pad_sequences(x_train, maxlen=100) # Pad IMDB training data with specified maxlen=100\n",
        "x_val = keras.preprocessing.sequence.pad_sequences(x_val, maxlen=100) # Pad IMDB validation data with specified maxlen=100\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_JFQeWK18SR"
      },
      "source": [
        "### 2.2 - Design and train LSTM model (25 Points) ###\n",
        "\n",
        "Build an LSTM model.\n",
        "\n",
        "**Tasks:**\n",
        "1. Build the LSTM model with **1 embedding layer**, **1 LSTM layer**, and **1 Dense layer**. **Print out** model summary. The embedding vector is specified with the dimension of **8**. **(10 Points)**\n",
        "2. Compile the LSTM model with **Adam** optimizer, **binary_crossentropy** loss function, and **accuracy** metrics. **(5 Points)**  \n",
        "3. Train the LSTM model with **batch_size=64 for 10 epochs** and report **training and validation accuracies over epochs**. **(5 Points)**\n",
        "4. **Print out** best validation accuracy. **(5 Points)**\n",
        "\n",
        "\n",
        "\n",
        "**Hints:**  \n",
        "1. Set input dimension to **1000** and output dimension to **8** for embedding layer.\n",
        "2. Set **unit_size=8** for LSTM layer.\n",
        "3. Set activation function to **sigmoid** for Dense layer.\n",
        "4. For validation: the outputs for first epoch might be like （but not need to be exactly） the statistics below:\\\n",
        "- **- loss: 0.5679 - accuracy: 0.7068 - val_loss: 0.4441 - val_accuracy: 0.8072**\n",
        "5. The model summary is as follows:\n",
        "- Total params: 8,553\n",
        "- Trainable params: 8,553\n",
        "- Non-trainable params: 0\n",
        "\n",
        "**Useful Reference:**\n",
        "1. https://keras.io/examples/nlp/bidirectional_lstm_imdb/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UDqqgFt118SS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25c0497f-b8b0-450c-d940-676a8cd91805"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, None, 8)           8000      \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 8)                 544       \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 9         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 8,553\n",
            "Trainable params: 8,553\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "782/782 [==============================] - 13s 11ms/step - loss: 0.5108 - accuracy: 0.7452 - val_loss: 0.4257 - val_accuracy: 0.8045\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fde8c11c290>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "### Model design with Embedding and LSTM layers ####\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int32\") # This is an easy way to set an adaptive length for input sequence\n",
        "x = layers.Embedding(input_dim=1000, output_dim=8)(inputs) # Embed data in an 8-dimensional vector\n",
        "x = layers.LSTM(units=8)(x) # Add 1st layer of LSTM with 8 hidden states (aka units)\n",
        "outputs = layers.Dense(units=1, activation='sigmoid')(x) # Add a classifier with units=1 and activation=\"sigmoid\"\n",
        "\n",
        "### Clear cached model to refresh memory and build new model for training ###\n",
        "keras.backend.clear_session() # Clear cached model\n",
        "model = keras.Model(inputs, outputs) # Build new keras model\n",
        "model.summary() # Print out model summary\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) # Compile built model with \"adam\", \"binary_crossentropy\", and metrics=[\"accuracy\"]\n",
        "model.fit(x_train, y_train, validation_data=(x_val, y_val)) # Train the compiled model with model.fit()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vqvy2tdEw7J"
      },
      "source": [
        "### 2.3 - LSTM hyperparameter tuning (Bonus 15 Points) ###\n",
        "\n",
        "Boost the performance of obtained LSTM (aka vanilla model) by hyperparameter tuning.\n",
        "\n",
        "**Tasks:**\n",
        "Note: \n",
        "- All modificiations are directly conducted based on the vanilla model above (from 2.2).\n",
        "- For each scenario, **report <span style=\"color:red\"> BEST Validation Accuracy </span> and generate Training/Validation <span style=\"color:red\"> Accuracy plots over epochs</span>**. You may just paste the plot figures in the cells with **Markdown mode**, or leave the result after running. **Make sure it is already correctly shown in your submitted file.**\n",
        "1.  Scenario 1 (**5 points**):\n",
        "    - Add one additional LSTM layer (totally 2 LSTM layers).\n",
        "    - Modify the embedding dimension to 16.\n",
        "    - Modify the units of LSTM to 16.\n",
        "2. Scenario 2 (**5 points**)\n",
        "    - Add one additional LSTM layer (totally 2 LSTM layers).\n",
        "    - Modify the embedding dimension to 128.\n",
        "    - Modify the units of LSTM to 128.\n",
        "3. Scenario 3 (**5 points**)\n",
        "    - Add one additional LSTM layer (totally 2 LSTM layers).\n",
        "    - Modify the embedding dimension to 128.\n",
        "    - Modify the units of LSTM to 128.\n",
        "    - Increase analysis length for review data to maxlen = 200\n",
        "\n",
        "**Hints:**  \n",
        "For validation: the outputs for first few epoches might be like （but not need to be exactly） the statistics below:\n",
        "- Scenario 1: **loss: 0.4968 - accuracy: 0.7450 - val_loss: 0.4079 - val_accuracy: 0.8198**\n",
        "- Scenario 2: **loss: 0.3883 - accuracy: 0.8286 - val_loss: 0.3844 - val_accuracy: 0.8242**\n",
        "- Scenario 3: **loss: 0.3688 - accuracy: 0.8382 - val_loss: 0.3692 - val_accuracy: 0.8333**\n",
        "\n",
        "- Summary of Model 1: Total params: 20,241; Trainable params: 20,241; Non-trainable params: 0\n",
        "- Summary of Model 2: Total params: 391,297; Trainable params: 391,297; Non-trainable params: 0\n",
        "- Summary of Model 3: Total params: 391,297; Trainable params: 391,297; Non-trainable params: 0\n",
        "\n",
        "You may follow the example from the reference below to add additional LSTM layer.\n",
        "\n",
        "**Useful Reference:**\n",
        "1. https://keras.io/examples/nlp/bidirectional_lstm_imdb/  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6xMSM_GQt_P8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e358346-4905-4017-88a9-265f7f9991b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, None, 16)          16000     \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, None, 16)          2112      \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 16)                2112      \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 20,241\n",
            "Trainable params: 20,241\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "391/391 [==============================] - 11s 20ms/step - loss: 0.4909 - accuracy: 0.7496 - val_loss: 0.4070 - val_accuracy: 0.8214\n",
            "Epoch 2/10\n",
            "391/391 [==============================] - 6s 17ms/step - loss: 0.3877 - accuracy: 0.8287 - val_loss: 0.3919 - val_accuracy: 0.8193\n",
            "Epoch 3/10\n",
            "391/391 [==============================] - 7s 17ms/step - loss: 0.3708 - accuracy: 0.8357 - val_loss: 0.3714 - val_accuracy: 0.8321\n",
            "Epoch 4/10\n",
            "391/391 [==============================] - 7s 18ms/step - loss: 0.3583 - accuracy: 0.8411 - val_loss: 0.3753 - val_accuracy: 0.8274\n",
            "Epoch 5/10\n",
            "391/391 [==============================] - 7s 17ms/step - loss: 0.3491 - accuracy: 0.8442 - val_loss: 0.3707 - val_accuracy: 0.8320\n",
            "Epoch 6/10\n",
            "391/391 [==============================] - 10s 24ms/step - loss: 0.3402 - accuracy: 0.8491 - val_loss: 0.4193 - val_accuracy: 0.8199\n",
            "Epoch 7/10\n",
            "391/391 [==============================] - 6s 16ms/step - loss: 0.3306 - accuracy: 0.8508 - val_loss: 0.3687 - val_accuracy: 0.8324\n",
            "Epoch 8/10\n",
            "391/391 [==============================] - 6s 16ms/step - loss: 0.3235 - accuracy: 0.8577 - val_loss: 0.3686 - val_accuracy: 0.8333\n",
            "Epoch 9/10\n",
            "391/391 [==============================] - 6s 16ms/step - loss: 0.3157 - accuracy: 0.8574 - val_loss: 0.3768 - val_accuracy: 0.8344\n",
            "Epoch 10/10\n",
            "391/391 [==============================] - 6s 16ms/step - loss: 0.3086 - accuracy: 0.8648 - val_loss: 0.3969 - val_accuracy: 0.8329\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fde8c428d90>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "########################### Scenario 1 ###########################\n",
        "##################################################################\n",
        "\n",
        "### Set random seed to ensure deterministic results ###\n",
        "import os\n",
        "seed_value = 1\n",
        "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
        "def reset_random_seeds():\n",
        "   tf.random.set_seed(seed_value)\n",
        "   np.random.seed(seed_value)\n",
        "   random.seed(seed_value)\n",
        "\n",
        "reset_random_seeds() # randomly set initial data\n",
        "\n",
        "max_features = 1000 # Only consider the top 1k words\n",
        "maxlen = 100 # Only consider the first 100 words of each movie review\n",
        "\n",
        "### Model design with Embedding and LSTM layers ####\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int32\") # This is an easy way to set an adaptive length for input sequence\n",
        "x = layers.Embedding(input_dim=1000, output_dim=16)(inputs) # Embed data in a 16-dimensional vector\n",
        "x = layers.LSTM(units=16, return_sequences=True)(x) # Add 1st layer of LSTM with 16 hidden states (aka units); set return_sequences=true.\n",
        "x = layers.LSTM(units=16)(x) # Add 2nd layer of LSTM with 16 hidden states (aka units)\n",
        "outputs = layers.Dense(units=1, activation='sigmoid')(x) # Add a classifier with units=1 and activation=\"sigmoid\"\n",
        "\n",
        "### Clear cached model to refresh memory and build new model for training ###\n",
        "keras.backend.clear_session() # Clear cached model\n",
        "model = keras.Model(inputs, outputs) # Build new keras model\n",
        "model.summary() # Print out model summary\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) # Compile built model with \"adam\", \"binary_crossentropy\", and metrics=[\"accuracy\"]\n",
        "model.fit(x_train, y_train, validation_data=(x_val, y_val), batch_size=64, epochs=10) # Train the compiled model using model.fit() with batch_size=64, epochs=10, and validation_data=(x_val, y_val)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Keod5xXkEKnx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96c09e22-f65f-4c26-f6ef-3b20fe225ad0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, None, 128)         128000    \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, None, 128)         131584    \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 128)               131584    \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 129       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 391,297\n",
            "Trainable params: 391,297\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "391/391 [==============================] - 14s 29ms/step - loss: 0.4616 - accuracy: 0.7772 - val_loss: 0.3900 - val_accuracy: 0.8262\n",
            "Epoch 2/10\n",
            "391/391 [==============================] - 8s 22ms/step - loss: 0.3775 - accuracy: 0.8340 - val_loss: 0.3805 - val_accuracy: 0.8257\n",
            "Epoch 3/10\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.3552 - accuracy: 0.8422 - val_loss: 0.3683 - val_accuracy: 0.8331\n",
            "Epoch 4/10\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.3318 - accuracy: 0.8544 - val_loss: 0.3644 - val_accuracy: 0.8387\n",
            "Epoch 5/10\n",
            "391/391 [==============================] - 9s 24ms/step - loss: 0.3152 - accuracy: 0.8644 - val_loss: 0.3592 - val_accuracy: 0.8422\n",
            "Epoch 6/10\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.2962 - accuracy: 0.8756 - val_loss: 0.3952 - val_accuracy: 0.8253\n",
            "Epoch 7/10\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.2827 - accuracy: 0.8798 - val_loss: 0.3811 - val_accuracy: 0.8352\n",
            "Epoch 8/10\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.2681 - accuracy: 0.8885 - val_loss: 0.3774 - val_accuracy: 0.8403\n",
            "Epoch 9/10\n",
            "391/391 [==============================] - 9s 24ms/step - loss: 0.2564 - accuracy: 0.8932 - val_loss: 0.3807 - val_accuracy: 0.8315\n",
            "Epoch 10/10\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.2392 - accuracy: 0.9019 - val_loss: 0.3992 - val_accuracy: 0.8346\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fde86f45290>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "########################### Scenario 2 ###########################\n",
        "##################################################################\n",
        "\n",
        "### Set random seed to ensure deterministic results ###\n",
        "import os\n",
        "seed_value = 1\n",
        "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
        "def reset_random_seeds():\n",
        "   tf.random.set_seed(seed_value)\n",
        "   np.random.seed(seed_value)\n",
        "   random.seed(seed_value)\n",
        "\n",
        "reset_random_seeds() # randomly set initial data\n",
        "\n",
        "max_features = 1000  # Only consider the top 1k words\n",
        "maxlen = 100 # Only consider the first 100 words of each movie review\n",
        "\n",
        "### Model design with Embedding and LSTM layers ####\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int32\") # This is an easy way to set an adaptive length for input sequence\n",
        "x = layers.Embedding(input_dim=1000, output_dim=128)(inputs) # Embed data in a 128-dimensional vector\n",
        "x = layers.LSTM(units=128, return_sequences=True)(x) # Add 1st layer of LSTM with 128 hidden states (aka units); set return_sequences=true.\n",
        "x = layers.LSTM(units=128)(x) # Add 2nd layer of LSTM with 128 hidden states (aka units)\n",
        "outputs = layers.Dense(units=1, activation='sigmoid')(x) # Add a classifier with units=1 and activation=\"sigmoid\"\n",
        "\n",
        "### Clear cached model to refresh memory and build new model for training ###\n",
        "keras.backend.clear_session() # Clear cached model\n",
        "model = keras.Model(inputs, outputs) # Build new keras model\n",
        "model.summary() # Print out model summary\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) # Compile built model with \"adam\", \"binary_crossentropy\", and metrics=[\"accuracy\"]\n",
        "model.fit(x_train, y_train, validation_data=(x_val, y_val), batch_size=64, epochs=10) # Train the compiled model using model.fit() with batch_size=64, epochs=10, and validation_data=(x_val, y_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DdiZbuCQt_QC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cece19e5-8d06-4ab9-a410-24c09bfc7484"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, None, 128)         128000    \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, None, 128)         131584    \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 128)               131584    \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 129       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 391,297\n",
            "Trainable params: 391,297\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "391/391 [==============================] - 12s 24ms/step - loss: 0.4616 - accuracy: 0.7772 - val_loss: 0.3900 - val_accuracy: 0.8263\n",
            "Epoch 2/10\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.3775 - accuracy: 0.8340 - val_loss: 0.3805 - val_accuracy: 0.8255\n",
            "Epoch 3/10\n",
            "391/391 [==============================] - 9s 23ms/step - loss: 0.3551 - accuracy: 0.8424 - val_loss: 0.3661 - val_accuracy: 0.8347\n",
            "Epoch 4/10\n",
            "391/391 [==============================] - 10s 25ms/step - loss: 0.3331 - accuracy: 0.8543 - val_loss: 0.3659 - val_accuracy: 0.8385\n",
            "Epoch 5/10\n",
            "391/391 [==============================] - 9s 23ms/step - loss: 0.3143 - accuracy: 0.8648 - val_loss: 0.3585 - val_accuracy: 0.8434\n",
            "Epoch 6/10\n",
            "391/391 [==============================] - 9s 23ms/step - loss: 0.2959 - accuracy: 0.8748 - val_loss: 0.4014 - val_accuracy: 0.8252\n",
            "Epoch 7/10\n",
            "391/391 [==============================] - 9s 23ms/step - loss: 0.2808 - accuracy: 0.8816 - val_loss: 0.3770 - val_accuracy: 0.8381\n",
            "Epoch 8/10\n",
            "391/391 [==============================] - 9s 24ms/step - loss: 0.2662 - accuracy: 0.8885 - val_loss: 0.3698 - val_accuracy: 0.8411\n",
            "Epoch 9/10\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.2496 - accuracy: 0.8975 - val_loss: 0.3855 - val_accuracy: 0.8293\n",
            "Epoch 10/10\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.2333 - accuracy: 0.9041 - val_loss: 0.3865 - val_accuracy: 0.8375\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fde64cf2190>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "########################### Scenario 3 ###########################\n",
        "##################################################################\n",
        "\n",
        "### Set random seed to ensure deterministic results ###\n",
        "import os\n",
        "seed_value = 1\n",
        "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
        "def reset_random_seeds():\n",
        "   tf.random.set_seed(seed_value)\n",
        "   np.random.seed(seed_value)\n",
        "   random.seed(seed_value)\n",
        "\n",
        "reset_random_seeds() # randomly set initial data\n",
        "\n",
        "max_features =  1000 # Only consider the top 1k words\n",
        "maxlen = 200 # Only consider the first 200 words of each movie review\n",
        "\n",
        "### Model design with Embedding and LSTM layers ####\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int32\") # This is an easy way to set an adaptive length for input sequence\n",
        "x = layers.Embedding(input_dim=1000, output_dim=128)(inputs) # Embed data in a 128-dimensional vector\n",
        "x = layers.LSTM(units=128, return_sequences=True)(x) # Add 1st layer of LSTM with 128 hidden states (aka units); set return_sequences=true.\n",
        "x = layers.LSTM(units=128)(x) # Add 2nd layer of LSTM with 128 hidden states (aka units)\n",
        "outputs = layers.Dense(units=1, activation='sigmoid')(x) # Add a classifier with units=1 and activation=\"sigmoid\"\n",
        "\n",
        "### Clear cached model to refresh memory and build new model for training ###\n",
        "keras.backend.clear_session() # Clear cached model\n",
        "model = keras.Model(inputs, outputs) # Build new keras model\n",
        "model.summary() # Print out model summary\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) # Compile built model with \"adam\", \"binary_crossentropy\", and metrics=[\"accuracy\"]\n",
        "model.fit(x_train, y_train, validation_data=(x_val, y_val), batch_size=64, epochs=10) # Train the compiled model using model.fit() with batch_size=64, epochs=10, and validation_data=(x_val, y_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oRu2xHOIt_QE"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "coursera": {
      "course_slug": "neural-networks-deep-learning",
      "graded_item_id": "XaIWT",
      "launcher_item_id": "zAgPl"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6+"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}